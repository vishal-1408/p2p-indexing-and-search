## (3) Querying with Hyperbee -- Secondary Indexing
In the previous exercise, we learned how to query for a range of dictionary entries, sorted by their names. Since the name is the primary key, we can use it in range queries efficiently.

Searching for definitions according to some other property, such as the definition's length, is still possible, but it would require scanning the entire database, because the key does not tell us anything about the definition.

## Exercises

Here we're going to search for dictionary entries that have a given length. Our current dictionary database isn't going to be well-suited for this, but let's try anyway. In Exercise 2, we'll see how the database can be extended to make these kinds of queries fast (hint: it will require adding a lot more keys).

We'll be extending the work you did in Problem 2 here.

#### Exercise 2

Let's try to find all definitions with a character length of 36, using the database from the previous exercise:

```js
async function findDefinitionsWithLength (length) {
  const matches = []
  for await (const result of db.createReadStream()) {
    if (result.value.length === length) matches.push(result)
  }
}
```

This is a pretty bad solution for a number of reasons! Not only is it slow, but every single reader of our database is going to have to do this full scan themselves, each time they want to run the query. Sure, the database will be cached locally after the first scan, so the bandwidth/latency cost will only be incurred once, but even locally this is far slower than it needs to be.

#### Exercise 2

What if instead of putting the burden on the reader to scan the full database, the writer did the complete scan once, then added additional "helper" kv-pairs to the database? These additional pairs just need to sort in a way that makes the definition length query fast. This is called "secondary indexing".

Generate a secondary index on definition lengths by iterating over the entire database, and for each entry inserting a kv-pair of the form:
```js
    length/name -> definition
```

As an example, if you come across the record `dog -> 'A four-legged animal'`, you'll want to create the new entry `20/dog -> 'A four-legged animal'`. You'll be duplicating definitions, but that's just fine.

As with the previous exercise, you'll probably want to speed this up by using the `batch` method.

Once the secondary index has been created, try answering the following queries (they should be almost instantaneous!):
1. "All definitions with length 35" (should be 1128 results, and compare how long this one takes against the full scan you did in Exercise 1) 
2. "All definitions with length 45 that start with 'b'" (should be 27 results)
3. "The first 10 definitions with length 22"

If you aren't sure how to translate those queries into `createReadStream` options, expand the hint below.

<details>
    <summary>Query Options</summary>

1. `{ gt: '35/', lt: '36/' }`
2. `{ gt: '45/b' lt: '45/c' }`
3. `{ gt: '22/', lt: '23/', limit: 10 }`
</details>

#### Bonus Exercise

In the previous exercise you duplicated the definitions in the secondary index. Try storing a pointer to the original entry instead, to save some space. Try imagining how many network requests a reader will need to do to answer a query, both with and without this indirection.

## Stuck?

Take a look at the [solution here](/solutions/03/index.js).

## Takeaways

Secondary indexing is essential for supporting different types of queries. Each index is usually tailored to a particular set of query types, so the more secondary indexes you add, the more types of queries you can answer quickly. This requires a lot of space though -- the more secondary indexes you add, the larger your database becomes.

Fortunately for us, sparse syncing means that even if the database is packed full of secondary indexes (huge!), readers will still only download the bits they need, when they need them.

There's a bug in our solution to these exercises though...

[Fix it by learning about key encodings in Problem 4](04.md)